{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kernel Flows for Irregular Time Series.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO39d+Pfr4XC16fkVtX2c+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlee1998/Advanced-Project-in-Mathematics/blob/main/Kernel_Flows_for_Irregular_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_zdNLOIqyWt",
        "outputId": "44d20fd8-4461-4fba-a744-cea414bf3473"
      },
      "source": [
        "import autograd.numpy as np\n",
        "import math\n",
        "#%% Kernel operations\n",
        "\n",
        "# Returns the norm of the pairwise difference\n",
        "def norm_matrix(matrix_1, matrix_2):\n",
        "    norm_square_1 = np.sum(np.square(matrix_1), axis = 1)\n",
        "    norm_square_1 = np.reshape(norm_square_1, (-1,1))\n",
        "    \n",
        "    norm_square_2 = np.sum(np.square(matrix_2), axis = 1)\n",
        "    norm_square_2 = np.reshape(norm_square_2, (-1,1))\n",
        "    \n",
        "    d1=matrix_1.shape\n",
        "    d2=matrix_2.shape\n",
        "#    print(d1)\n",
        "#    print(d2)\n",
        "    if d1[1]!=d2[1]:\n",
        "        matrix_1=np.transpose(matrix_1)\n",
        "    \n",
        "    inner_matrix = np.matmul(matrix_1, np.transpose(matrix_2))\n",
        "    \n",
        "    norm_diff = -2 * inner_matrix + norm_square_1 + np.transpose(norm_square_2)\n",
        "#    print(norm_diff.shape)\n",
        "    \n",
        "    return norm_diff\n",
        "\n",
        "# Returns the pairwise inner product\n",
        "def inner_matrix(matrix_1, matrix_2):\n",
        "    d1=matrix_1.shape\n",
        "    d2=matrix_2.shape\n",
        "    # print(d1)\n",
        "    # print(d2)\n",
        "    if d1[1]!=d2[1]:\n",
        "        matrix_1=np.transpose(matrix_1)\n",
        "    return np.matmul(matrix_1, np.transpose(matrix_2))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('This is the matrix operations file')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the matrix operations file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsfpcEx9qFOv",
        "outputId": "67f13358-ad37-4a1b-cca9-a7ed34121528"
      },
      "source": [
        "\"\"\" In this section we define various kernels. Warning, not all of them work \n",
        "at the moment, the most reliable one is the RBF kernel. Note that currently the \n",
        "laplacian kernel does not work\"\"\"\n",
        "        \n",
        "\n",
        "# Define the RBF Kernel. Takes an array of parameters, returns a value\n",
        "def kernel_RBF(matrix_1, matrix_2, parameters):\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    sigma = parameters[0]\n",
        "    K =  np.exp(-matrix/(2*1.0**2))\n",
        "    \n",
        "    return K\n",
        "\n",
        "\n",
        "# do not use right now\n",
        "def kernel_laplacian(matrix_1, matrix_2, parameters):\n",
        "    gamma = parameters[0]\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    K =  np.exp(-matrix * gamma)\n",
        "    return K\n",
        "\n",
        "def kernel_sigmoid(matrix_1, matrix_2, parameters):\n",
        "    alpha = parameters[0]\n",
        "    beta = parameters[1]\n",
        "    matrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = np.tanh(alpha *matrix + beta)\n",
        "    return K\n",
        "\n",
        "def kernel_rational_quadratic(matrix_1, matrix_2, parameters):\n",
        "    alpha = parameters[0]\n",
        "    beta = parameters[1]\n",
        "    epsilon = 0.0001\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    return (beta**2 + matrix)**(-(alpha+ epsilon))\n",
        "\n",
        "def kernel_inverse_power_alpha(matrix_1, matrix_2, parameters):\n",
        "    alpha = parameters[0]\n",
        "    beta = 1.0\n",
        "    epsilon = 0.0001\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    return (beta**2 + matrix)**(-(alpha+ epsilon))\n",
        "\n",
        "def kernel_inverse_multiquad(matrix_1, matrix_2, parameters):\n",
        "    beta = parameters[0]\n",
        "    gamma = parameters[1]\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    return (beta**2 + gamma*matrix)**(-1/2)\n",
        "\n",
        "def kernel_cauchy(matrix_1, matrix_2, parameters):\n",
        "    sigma = parameters[0]\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    return 1/(1 + matrix/sigma**2)\n",
        "\n",
        "def kernel_quad(matrix_1, matrix_2, parameters):\n",
        "    c = parameters[0]\n",
        "    matrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = (matrix+c) ** 2\n",
        "    return K \n",
        "\n",
        "def kernel_poly(matrix_1, matrix_2, parameters):\n",
        "    a = parameters[0]\n",
        "    b = parameters[1]\n",
        "    d = parameters[2]\n",
        "    matrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = (a * matrix + b) ** d\n",
        "    return K \n",
        "\n",
        "\n",
        "def kernel_gaussian_linear(matrix_1, matrix_2, parameters):\n",
        "    K = 0\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    for i in range(parameters.shape[1]):\n",
        "        # print(\"beta\", parameters[1, i])\n",
        "        # print(\"sigma\", parameters[0, i])\n",
        "        K = K + parameters[1, i]**2*np.exp(-matrix / (2* parameters[0, i]**2))\n",
        "    return K\n",
        "\n",
        "\n",
        "\n",
        "def kernel_anl(matrix_1, matrix_2, parameters):\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    sigma = parameters[0]\n",
        "    K =  np.exp(-matrix/ (2* sigma**2))\n",
        "    K=K*(parameters[1])**2\n",
        "    \n",
        "    c = (parameters[2])**2\n",
        "    matrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = K+ (parameters[3])**2 *(matrix+c) ** 2\n",
        "    \n",
        "    beta = parameters[4]\n",
        "    gamma = (parameters[5])**2\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    K=K+ (parameters[6])**2 *(beta**2 + gamma*matrix)**(-1/2)\n",
        "    \n",
        "    alpha = parameters[7]\n",
        "    beta = parameters[8]\n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    K=K+ (parameters[9])**2 *(beta**2 + matrix)**(-alpha)\n",
        "    \n",
        "    return K\n",
        "\n",
        "def kernel_anl2(matrix_1, matrix_2, parameters):\n",
        "    i=0\n",
        "    \n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    sigma = parameters[i+0]\n",
        "    K =  np.exp(-matrix/ (2* sigma**2))\n",
        "    K=K*(parameters[i+1])**2\n",
        "    i=i+2\n",
        "    \n",
        "    \n",
        "    c = (parameters[i])**2\n",
        "    imatrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = K+ (parameters[i+1])**2 *(imatrix+c) ** 2\n",
        "    i=i+2\n",
        "    \n",
        "    beta = parameters[i]\n",
        "    gamma = (parameters[i+1])**2\n",
        "    K=K+ (parameters[i+2])**2 *(beta**2 + gamma*matrix)**(-1/2)\n",
        "    i=i+3\n",
        "    \n",
        "    alpha = parameters[i]\n",
        "    beta = parameters[i+1]\n",
        "    K=K+ (parameters[i+2])**2 *(beta**2 + matrix)**(-alpha)\n",
        "    i=i+3\n",
        "    \n",
        "    return K\n",
        "\n",
        "\n",
        "def kernel_anl3(matrix_1, matrix_2, parameters):\n",
        "    i=0\n",
        "    \n",
        "    matrix = norm_matrix(matrix_1, matrix_2)\n",
        "    sigma = parameters[i+0]\n",
        "    K =  np.exp(-matrix/ (2* sigma**2))\n",
        "    K=K*(parameters[i+1])**2\n",
        "    i=i+2\n",
        "    \n",
        "    \n",
        "    c = (parameters[i])**2\n",
        "    imatrix = inner_matrix(matrix_1, matrix_2)\n",
        "    K = K+ (parameters[i+1])**2 *(imatrix+c) ** 2\n",
        "    i=i+2\n",
        "    \n",
        "    beta = parameters[i]\n",
        "    gamma = (parameters[i+1])**2\n",
        "    K=K+ (parameters[i+2])**2 *(beta**2 + gamma*matrix)**(-1/2)\n",
        "    i=i+3\n",
        "    \n",
        "    alpha = parameters[i]\n",
        "    beta = parameters[i+1]\n",
        "    K=K+ (parameters[i+2])**2 *(beta**2 + matrix)**(-alpha)\n",
        "    i=i+3\n",
        "    \n",
        "    sigma = parameters[i]\n",
        "    K=K+ (parameters[i+1])**2 * 1/(1 + matrix/sigma**2)\n",
        "    i=i+2\n",
        "    \n",
        "    alpha_0 = parameters[i]\n",
        "    sigma_0 = parameters[i+1]\n",
        "    alpha_1 = parameters[i+2]\n",
        "    sigma_1 = parameters[i+3]\n",
        "    K =  K+ (parameters[i+4])**2 *alpha_0*np.maximum(0, 1-matrix/(sigma_0))+ alpha_1 * np.exp(-matrix/ (2* sigma_1**2))\n",
        "    i=i+5\n",
        "    \n",
        "    p = parameters[i]\n",
        "    l = parameters[i+1]\n",
        "    sigma = parameters[i+2]\n",
        "    K =K+ (parameters[i+3])**2 * np.exp(-np.sin(matrix*np.pi/p)**2/l**2)*np.exp(-matrix/sigma**2)\n",
        "    i=i+4\n",
        "    \n",
        "    p = parameters[i]\n",
        "    l = parameters[i+1]\n",
        "    K = K+ (parameters[i+2])**2 *np.exp(-np.sin(matrix*np.pi/p)/l**2)\n",
        "    i=i+3\n",
        "    \n",
        "\n",
        "    return K\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"A dictionnary containing the different kernels. If you wish to build a custom \n",
        " kernel, add the function to the dictionnary.\n",
        "\"\"\"\n",
        "kernels_dic = {\"RBF\" : kernel_RBF,\"poly\": kernel_poly, \"Laplacian\": kernel_laplacian, \n",
        "               \"sigmoid\": kernel_sigmoid, \"rational quadratic\": kernel_rational_quadratic,\n",
        "               \"inverse_multiquad\": kernel_inverse_multiquad, \"quadratic\" : kernel_quad,\n",
        "               \"poly\": kernel_poly, \"inverse_power_alpha\": kernel_inverse_power_alpha,\n",
        "               \"gaussian multi\": kernel_gaussian_linear, \"anl\": kernel_anl, \"anl2\": kernel_anl2,\n",
        "               \"anl3\": kernel_anl3}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('This is the kernel file')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the kernel file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h68WIrgWvFGy",
        "outputId": "afffcc7e-4464-4ec2-a1b1-c6deaf7cc501"
      },
      "source": [
        "!git clone https://github.com/KalmanNet/KalmanNet_TSP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KalmanNet_TSP'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 50 (delta 13), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgN3rfZ-q2CA"
      },
      "source": [
        "def Henon(T, dt, N_sims,a,b):\n",
        "    N_t  = int(T//dt)\n",
        "    sims = np.zeros((N_sims, N_t, 2))\n",
        "    for i in range(1,N_t):\n",
        "      sims[:, i] = np.array([1-a*sims[:,i-1,0]**2+sims[:,i-1,1],b*sims[:,i-1,0]]).T\n",
        "    return sims.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwSOznqfq7AW"
      },
      "source": [
        "# generate dataset\n",
        "Data = Henon(T=10, dt=0.005,N_sims=1,a=1.4,b=0.3)[0]\n",
        "#train test split\n",
        "train_data = Data[:600,:]\n",
        "test_data = Data[600:,:].T\n",
        "#times\n",
        "Times = np.linspace(1,2000-1,2000-1)\n",
        "#take random subset\n",
        "indices_random = np.sort(np.random.choice(600,360,replace=False))\n",
        "#take train points corresponding to subset indices\n",
        "train_times = Times[indices_random]\n",
        "train_data = train_data[indices_random,:].T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOXiyde37gkk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jbHE4YWrDHs"
      },
      "source": [
        "np.random.seed(1)\n",
        "from autograd import value_and_grad \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "def sample_selection(data, size):\n",
        "  indices = np.arange(data.shape[0])\n",
        "  sample_indices = np.sort(np.random.choice(indices, size, replace= False)) \n",
        "  return sample_indices\n",
        "\n",
        "def batch_creation(data, batch_size, sample_proportion = 0.5):\n",
        "  if batch_size == False:\n",
        "    data_batch = data\n",
        "    batch_indices = np.arange(data.shape[0])\n",
        "  elif 0 < batch_size <= 1:\n",
        "    batch_size = int(data.shape[0] * batch_size)\n",
        "    batch_indices = sample_selection(data, batch_size)\n",
        "    data_batch = data[batch_indices]\n",
        "  else:\n",
        "    batch_indices = sample_selection(data, batch_size)\n",
        "    data_batch = data[batch_indices]\n",
        "        \n",
        "\n",
        "    # Sample from the mini-batch\n",
        "  sample_size = math.ceil(data_batch.shape[0]*sample_proportion)\n",
        "  sample_indices = sample_selection(data_batch, sample_size)\n",
        "    \n",
        "  return sample_indices, batch_indices\n",
        "\n",
        "\n",
        "# Generate a prediction\n",
        "def kernel_regression(X_train, X_test, Y_train, param, kernel_keyword = \"RBF\", regu_lambda = 0.000001):\n",
        "    kernel = kernels_dic[kernel_keyword]\n",
        "    k_matrix = kernel(X_train, X_train, param)\n",
        "    k_matrix += regu_lambda * np.identity(k_matrix.shape[0])\n",
        "    t_matrix = kernel(X_test, X_train, param) \n",
        "    prediction = np.matmul(t_matrix, np.matmul(np.linalg.inv(k_matrix), Y_train)) \n",
        "    return prediction\n",
        "\n",
        "# redicttimeseries\n",
        "\n",
        "def kernel_extrapolate(X_train, X_test, Y_train, param, nsteps=1, kernel_keyword = \"RBF\", regu_lambda = 0.000001):\n",
        "    kernel = kernels_dic[kernel_keyword]\n",
        "    k_matrix = kernel(X_train, X_train, param)\n",
        "    k_matrix += regu_lambda * np.identity(k_matrix.shape[0])\n",
        "    A=np.matmul(np.linalg.inv(k_matrix), Y_train)\n",
        "    arr = np.array([])\n",
        "    \n",
        "    X_test0=X_test\n",
        "    isteps=int(nsteps/(X_test.shape[1]))+1\n",
        "    for i in range(isteps):\n",
        "        X_test1=X_test0\n",
        "        t_matrix = kernel(X_test1, X_train, param) \n",
        "        prediction = np.matmul(t_matrix, A) \n",
        "        X_test0=prediction\n",
        "        arr = np.append(arr, np.array(prediction[0,:]))\n",
        "    arr=arr[0:nsteps]\n",
        "    return arr\n",
        "\n",
        "\n",
        "def replace_nan(array):\n",
        "    for i in range(array.shape[0]):\n",
        "        if math.isnan(array[i]) == True:\n",
        "            print(\"Found nan value, replacing by 0\")\n",
        "            array[i] = 0\n",
        "    return array\n",
        "\n",
        "def sample_size_linear(iterations, range_tuple):\n",
        "    \n",
        "    return np.linspace(range_tuple[0], range_tuple[1], num = iterations)[::-1]\n",
        "            \n",
        "#%% Rho function\n",
        "\n",
        "# The pi or selection matrix\n",
        "def pi_matrix(sample_indices, dimension):\n",
        "    pi = np.zeros(dimension)\n",
        "    \n",
        "    for i in range(dimension[0]):\n",
        "        pi[i][sample_indices[i]] = 1\n",
        "    \n",
        "    return pi\n",
        "\n",
        "\n",
        "def rho(parameters, matrix_data, Y_data, sample_indices,  kernel_keyword= \"RBF\", regu_lambda = 0.000001):\n",
        "    kernel = kernels_dic[kernel_keyword]\n",
        "    \n",
        "    kernel_matrix = kernel(matrix_data, matrix_data, parameters)\n",
        "#    print(kernel_matrix.shape)\n",
        "    pi = pi_matrix(sample_indices, (sample_indices.shape[0], matrix_data.shape[0]))   \n",
        "#    print(pi.shape)\n",
        "    \n",
        "    sample_matrix = np.matmul(pi, np.matmul(kernel_matrix, np.transpose(pi)))\n",
        "#    print(sample_matrix.shape)\n",
        "    \n",
        "    Y_sample = Y_data[sample_indices]\n",
        "#    print(Y_sample.shape)\n",
        "    \n",
        "    lambda_term = regu_lambda\n",
        "    inverse_data = np.linalg.inv(kernel_matrix + lambda_term * np.identity(kernel_matrix.shape[0]))\n",
        "    inverse_sample = np.linalg.inv(sample_matrix + lambda_term * np.identity(sample_matrix.shape[0]))\n",
        "#    print(inverse_sample.shape)\n",
        "#    B=np.matmul(inverse_sample, Y_sample)\n",
        "#    print(B.shape)\n",
        "    \n",
        "    top = np.tensordot(Y_sample, np.matmul(inverse_sample, Y_sample))\n",
        "    \n",
        "   \n",
        "    bottom = np.tensordot(Y_data, np.matmul(inverse_data, Y_data))\n",
        "    \n",
        "    #print(1-top/bottom)\n",
        "    return 1 - top/bottom\n",
        "\n",
        "def l2(parameters, matrix_data, Y, batch_indices, sample_indices, kernel_keyword = \"RBF\"):\n",
        "    X_sample = matrix_data[sample_indices]\n",
        "    Y_sample = Y[sample_indices]\n",
        "    \n",
        "    not_sample = [x for x in batch_indices not in sample_indices]\n",
        "    X_not_sample = matrix_data[not_sample]\n",
        "    Y_not_sample = Y[not_sample]\n",
        "    prediction = kernel_regression(X_sample, X_not_sample, Y_sample, kernel_keyword)\n",
        "    \n",
        "    return np.dot(Y_not_sample - prediction, Y_not_sample- prediction)\n",
        "\n",
        "#%% Grad functions\n",
        "\n",
        "\"\"\" We define the gradient calculator function.Like rho, the gradient \n",
        "calculator function accesses the gradfunctions via a keyword\"\"\"\n",
        "\n",
        "# Gradient calculator function. Returns an array\n",
        "def grad_kernel(parameters, X_data, Y_data, sample_indices, kernel_keyword= \"RBF\", regu_lambda = 0.000001):\n",
        "    grad_K = value_and_grad(rho)\n",
        "    rho_value, gradient = grad_K(parameters, X_data, Y_data, sample_indices, kernel_keyword, regu_lambda = regu_lambda)\n",
        "    return rho_value, gradient\n",
        "\n",
        "#%% The class version of KF\n",
        "class KernelFlowsP():\n",
        "    \n",
        "    def __init__(self, kernel_keyword, parameters):\n",
        "        self.kernel_keyword = kernel_keyword\n",
        "        self.parameters = np.copy(parameters)\n",
        "        \n",
        "        # Lists that keep track of the history of the algorithm\n",
        "        self.rho_values = []\n",
        "        self.grad_hist = []\n",
        "        self.para_hist = []\n",
        "        \n",
        "        self.LR = 0.1\n",
        "        self.beta = 0.9\n",
        "        self.regu_lambda = 0.0001\n",
        "    \n",
        "    def get_hist(self):\n",
        "        return self.param_hist, self.gradients, self.rho_values\n",
        "        \n",
        "    \n",
        "    def save_model(self):\n",
        "        np.save(\"param_hist\", self.param_hist)\n",
        "        np.save(\"gradients\", self.gradients)\n",
        "        np.save(\"rho_values\", self.rho_values)\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "    \n",
        "    def set_LR(self, value):\n",
        "        self.LR = value\n",
        "        \n",
        "    def set_beta(self, value):\n",
        "        self.beta = value\n",
        "    def set_train(self, train):\n",
        "        self.train = train\n",
        "        \n",
        "    \n",
        "    def fit(self, X, Y, iterations, batch_size = False, optimizer = \"SGD\", \n",
        "            learning_rate = 0.1, beta = 0.9, show_it = 100, regu_lambda = 0.000001, \n",
        "            adaptive_size = False, adaptive_range = (), proportion = 0.5, reduction_constant = 0.0):            \n",
        "\n",
        "        self.set_LR(learning_rate)\n",
        "        self.set_beta(beta)\n",
        "        self.regu_lambda = regu_lambda\n",
        "        \n",
        "        self.X_train = np.copy(X)\n",
        "        self.Y_train = np.copy(Y)\n",
        "        momentum = np.zeros(self.parameters.shape, dtype = \"float\")\n",
        "        \n",
        "        # This is used for the adaptive sample decay\n",
        "        rho_100 = []\n",
        "        adaptive_mean = 0\n",
        "        adaptive_counter = 0\n",
        "        \n",
        "        if adaptive_size == False or adaptive_size == \"Dynamic\":\n",
        "            sample_size = proportion\n",
        "        elif adaptive_size == \"Linear\":\n",
        "            sample_size_array = sample_size_linear(iterations, adaptive_range) \n",
        "        else:\n",
        "            print(\"Sample size not recognized\")\n",
        "            \n",
        "        for i in range(iterations-1):\n",
        "            if i % show_it == 0:\n",
        "                print(\"parameters \", self.parameters)\n",
        "            \n",
        "            if adaptive_size == \"Linear\":\n",
        "                sample_size = sample_size_array[i]\n",
        "                \n",
        "            elif adaptive_size == \"Dynamic\" and adaptive_counter == 100:\n",
        "                if adaptive_mean != 0:\n",
        "                    change = np.mean(rho_100) - adaptive_mean \n",
        "                else:\n",
        "                    change = 0\n",
        "                adaptive_mean = np.mean(rho_100)\n",
        "                rho_100 = []\n",
        "                sample_size += change - reduction_constant\n",
        "                adaptive_counter= 0\n",
        "                \n",
        "            # Create a batch and a sample\n",
        "            sample_indices, batch_indices = batch_creation(X, batch_size, sample_proportion = sample_size)\n",
        "            X_data = X[batch_indices]\n",
        "            Y_data = Y[batch_indices]\n",
        "            \n",
        "\n",
        "                \n",
        "            # Changes parameters according to SGD rules\n",
        "            if optimizer == \"SGD\":\n",
        "                rho, grad_mu = grad_kernel(self.parameters, X_data, Y_data, \n",
        "                                           sample_indices, self.kernel_keyword, regu_lambda = regu_lambda)\n",
        "                #print(grad_mu)\n",
        "                if  rho > 1 or rho < 0:\n",
        "                    print(\"Warning, rho outside [0,1]: \", rho)\n",
        "                else:\n",
        "                    self.parameters -= learning_rate * grad_mu \n",
        "                    #* (Times[i]-train_times[i])\n",
        "                    \n",
        "            \n",
        "            # Changes parameters according to Nesterov Momentum rules     \n",
        "            elif optimizer == \"Nesterov\":\n",
        "                rho, grad_mu = grad_kernel(self.parameters - learning_rate * beta * momentum, \n",
        "                                               X_data, Y_data, sample_indices, self.kernel_keyword, regu_lambda = regu_lambda)\n",
        "                if  rho > 1 or rho < 0:\n",
        "                    print(\"Warning, rho outside [0,1]: \", rho)\n",
        "                else:\n",
        "                    momentum = beta * momentum + grad_mu\n",
        "                    self.parameters -= learning_rate * momentum * (train_times[i]-train_times[i-1])\n",
        "                \n",
        "            else:\n",
        "                print(\"Error optimizer, name not recognized\")\n",
        "            \n",
        "            # Update history \n",
        "            self.para_hist.append(np.copy(self.parameters))\n",
        "            self.rho_values.append(rho)\n",
        "            self.grad_hist.append(np.copy(grad_mu))\n",
        "            \n",
        "            rho_100.append(rho)\n",
        "            adaptive_counter +=1\n",
        "                \n",
        "            \n",
        "        # Convert all the lists to np arrays\n",
        "        self.para_hist = np.array(self.para_hist) \n",
        "        self.rho_values = np.array(self.rho_values)\n",
        "        self.grad_hist = np.array(self.grad_hist)\n",
        "                \n",
        "        return self.parameters\n",
        "    \n",
        "    def predict(self,test, regu_lambda = 0.0000001):\n",
        "         \n",
        "        X_train = self.X_train\n",
        "        Y_train = self.Y_train\n",
        "        prediction = kernel_regression(X_train, test, Y_train, self.parameters, self.kernel_keyword, regu_lambda = regu_lambda) \n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def extrapolate(self,test, nsteps=1,regu_lambda = 0.000001):\n",
        "         \n",
        "        X_train = self.X_train\n",
        "        Y_train = self.Y_train\n",
        "        prediction = kernel_extrapolate(X_train, test, Y_train, self.parameters, nsteps,self.kernel_keyword, regu_lambda = regu_lambda) \n",
        "\n",
        "        return prediction\n",
        "\n",
        "def fit_data_anl3(train_data,delay,regu_lambda,noptsteps=100):\n",
        "    lenX=len(train_data[0,:])\n",
        "    num_modes = train_data.shape[0]\n",
        "\n",
        "    # Some constants\n",
        "    nparameters=24\n",
        "    vdelay=delay*np.ones((num_modes,), dtype=int)\n",
        "    vregu_lambda=regu_lambda*np.ones((num_modes,))\n",
        "\n",
        "    # Get scaling factor    \n",
        "    normalize=np.amax(train_data[:,:])\n",
        "\n",
        "    # Prepare training data\n",
        "    X=np.zeros((1+lenX-2*delay,delay*num_modes))\n",
        "    Y=np.zeros((1+lenX-2*delay,delay*num_modes))\n",
        "    for mode in range(train_data.shape[0]):\n",
        "        for i in range(1+lenX-2*delay):\n",
        "              X[i,(mode*delay):(mode*delay+delay)]=train_data[mode,i:(i+delay)]\n",
        "              Y[i,(mode*delay):(mode*delay+delay)]=train_data[mode,(i+delay):(i+2*delay)]\n",
        "\n",
        "    # Normalize\n",
        "    X=X/normalize\n",
        "    Y=Y/normalize     \n",
        "    \n",
        "    # Fit data\n",
        "    c=np.zeros(nparameters)+1\n",
        "    mu_1 = c\n",
        "    kerneltype=\"RBF\"\n",
        "    K = KernelFlowsP(kerneltype, mu_1)\n",
        "    mu_pred = K.fit(X, Y, noptsteps, optimizer = \"SGD\",  batch_size = 100, show_it = 500, regu_lambda=regu_lambda)\n",
        "    mu_1=mu_pred\n",
        "    c=mu_1\n",
        "\n",
        "    kernel = kernels_dic[kerneltype]\n",
        "    k_matrix = kernel(X, X, mu_1)\n",
        "    k_matrix += regu_lambda * np.identity(k_matrix.shape[0])\n",
        "    A=np.matmul(np.linalg.inv(k_matrix), Y)\n",
        "\n",
        "    return k_matrix, A, mu_1, normalize, X\n",
        "\n",
        "def fit_error_model_anl3(train_data,error_data,delay,regu_lambda,noptsteps=100):\n",
        "    lenX=len(train_data[0,:])\n",
        "    num_modes = train_data.shape[0]\n",
        "\n",
        "    # Some constants\n",
        "    nparameters=24\n",
        "    vdelay=delay*np.ones((num_modes,), dtype=int)\n",
        "    vregu_lambda=regu_lambda*np.ones((num_modes,))\n",
        "\n",
        "    # Get scaling factor    \n",
        "    normalize=np.amax(train_data[:,:])\n",
        "\n",
        "    # Prepare training data\n",
        "    X=np.zeros((1+lenX-2*delay,delay*num_modes))\n",
        "    Y=np.zeros((1+lenX-2*delay,delay*num_modes))\n",
        "    for mode in range(train_data.shape[0]):\n",
        "        for i in range(1+lenX-2*delay):\n",
        "              X[i,(mode*delay):(mode*delay+delay)]=train_data[mode,i:(i+delay)]\n",
        "              Y[i,(mode*delay):(mode*delay+delay)]=error_data[mode,(i+delay):(i+2*delay)]\n",
        "\n",
        "    # Normalize\n",
        "    X=X/normalize\n",
        "    Y=Y/normalize     \n",
        "    \n",
        "    # Fit data\n",
        "    c=np.zeros(nparameters)+1\n",
        "    mu_1 = c\n",
        "    kerneltype=\"RBF\"\n",
        "    K = KernelFlowsP(kerneltype, mu_1)\n",
        "    mu_pred = K.fit(X, Y, noptsteps, optimizer = \"Nesterov\",  batch_size = 100, show_it = 500, regu_lambda=regu_lambda)\n",
        "    mu_1=mu_pred\n",
        "    c=mu_1\n",
        "\n",
        "    kernel = kernels_dic[kerneltype]\n",
        "    k_matrix = kernel(X, X, mu_1)\n",
        "    k_matrix += regu_lambda * np.identity(k_matrix.shape[0])\n",
        "    A=np.matmul(np.linalg.inv(k_matrix), Y)\n",
        "\n",
        "    return k_matrix, A, mu_1, normalize, X\n",
        "\n",
        "def test_fit_anl3(test_data,train_X,delay,k_matrix,A,param,normalize):\n",
        "    lenXt=len(test_data[0,:])\n",
        "    num_modes = test_data.shape[0]\n",
        "\n",
        "    nsteps=lenXt\n",
        "    X_test=np.zeros((1,delay*num_modes))\n",
        "    X_test[0,:] = test_data[:,:delay].reshape(1,-1)\n",
        "    X_test=X_test/normalize\n",
        "    \n",
        "    arr = np.zeros((nsteps,num_modes))\n",
        "    arr[:2*delay,:] = train_data[:,:2*delay].T/normalize\n",
        "\n",
        "    kerneltype = \"RBF\"\n",
        "    kernel = kernels_dic[kerneltype]\n",
        "\n",
        "    X_test0=X_test\n",
        "    isteps=int(nsteps/delay)+1\n",
        "    for i in range(1,isteps):\n",
        "        X_test1=X_test0\n",
        "        t_matrix = kernel(X_test1, train_X, param) \n",
        "        prediction = np.matmul(t_matrix, A) \n",
        "        \n",
        "        # Just using true data\n",
        "        X_test0 = (test_data[:,i*delay:(i+1)*delay]/normalize).reshape(1,-1)\n",
        "\n",
        "        # # Feedback\n",
        "        # X_test0=prediction\n",
        "        \n",
        "        for mode in range(num_modes):\n",
        "            imin=i*delay\n",
        "            imax=np.minimum((i+1)*delay,nsteps)\n",
        "            delaymin=imax-imin\n",
        "            arr[imin:imax,mode]=np.array(prediction[0,(mode*delay):(mode*delay+delaymin)])\n",
        "\n",
        "    # Rescale\n",
        "    predall=arr*normalize\n",
        "\n",
        "    return predall.T\n",
        "\n",
        "def test_error_model_anl3(test_data,train_X,delay,k_matrix,A,param,normalize):\n",
        "    lenXt=len(test_data[0,:])\n",
        "    num_modes = test_data.shape[0]\n",
        "\n",
        "    nsteps=lenXt\n",
        "    X_test=np.zeros((1,delay*num_modes))\n",
        "    X_test[0,:] = test_data[:,:delay].reshape(1,-1)\n",
        "    X_test=X_test/normalize\n",
        "    \n",
        "    arr = np.zeros((nsteps,num_modes))\n",
        "    arr[:2*delay,:] = train_data[:,:2*delay].T/normalize\n",
        "\n",
        "    kerneltype = \"RBF\"\n",
        "    kernel = kernels_dic[kerneltype]\n",
        "\n",
        "    X_test0=X_test\n",
        "    isteps=int(nsteps/delay)+1\n",
        "    for i in range(1,isteps):\n",
        "        X_test1=X_test0\n",
        "        t_matrix = kernel(X_test1, train_X, param) \n",
        "        prediction = np.matmul(t_matrix, A) \n",
        "        \n",
        "        # Just using true data\n",
        "        X_test0 = (test_data[:,i*delay:(i+1)*delay]/normalize).reshape(1,-1)\n",
        "\n",
        "        # # Feedback\n",
        "        # X_test0=prediction\n",
        "        \n",
        "        for mode in range(num_modes):\n",
        "            imin=i*delay\n",
        "            imax=np.minimum((i+1)*delay,nsteps)\n",
        "            delaymin=imax-imin\n",
        "            arr[imin:imax,mode]=np.array(prediction[0,(mode*delay):(mode*delay+delaymin)])\n",
        "\n",
        "    # Rescale\n",
        "    predall=arr*normalize\n",
        "\n",
        "    return predall.T\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #train_data = np.load('POD_Coeffs_train.npy')\n",
        "    #test_data = np.load('POD_Coeffs_test.npy')\n",
        "\n",
        "    # Set delay\n",
        "    delay = 5\n",
        "    regu_lambda = 0.010\n",
        "    #noptsteps = 1000\n",
        "    noptsteps = len(train_times)\n",
        "\n",
        "    # Fit on training data\n",
        "    k_matrix, A, param, normalize, train_X = fit_data_anl3(train_data,delay,regu_lambda,noptsteps)\n",
        "\n",
        "    # Predict and get error on training data\n",
        "    predicted_train = test_fit_anl3(train_data, train_X, delay, k_matrix, A, param, normalize)\n",
        "\n",
        "\n",
        "    #np.save('RKHS_Train_Prediction.npy',predicted_train)\n",
        "    #train_error = np.abs(predicted_train-train_data)\n",
        "    #np.save('RKHS_Train_Error.npy',train_error)\n",
        "    test_data= Data[:600,:].T\n",
        "\n",
        "    # Predict testing data\n",
        "    predicted_test = test_fit_anl3(test_data, train_X, delay, k_matrix, A, param, normalize)\n",
        "    #np.save('RKHS_Test_Prediction.npy',predicted_test)\n",
        "    test_error = np.abs(predicted_test-test_data)\n",
        "   # np.save('RKHS_Test_Error.npy',train_error)\n",
        "\n",
        "    # Visualize the modal predictions\n",
        "    for mode_num in range(2):\n",
        "        fig, ax = plt.subplots(nrows=1,ncols=2)\n",
        "        ax[0].plot(predicted_test[mode_num,:],label='Test predicted')\n",
        "        ax[0].plot(test_data[mode_num,:],label='Test true')\n",
        "        ax[0].legend()\n",
        "\n",
        "        ax[1].plot(predicted_train[mode_num,:],label='Train predicted')\n",
        "        ax[1].plot(train_data[mode_num,:],label='Train true')\n",
        "        ax[1].legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    #exit()\n",
        "\n",
        "    # Fit an error model\n",
        "    #k_matrix_err, A_err, param_err, normalize_err, train_X_err = fit_error_model_anl3(train_data,train_error,delay,regu_lambda,noptsteps)\n",
        "\n",
        "    # Predict the error on train data\n",
        "    #predicted_train_error = test_error_model_anl3(train_data, train_X_err, \n",
        "    #                                    delay, k_matrix_err, A_err, param_err, normalize_err)\n",
        "\n",
        "    # Predict the error on test data\n",
        "    #predicted_test_error = test_error_model_anl3(test_data, train_X_err, \n",
        "    #                                    delay, k_matrix_err, A_err, param_err, normalize_err)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}